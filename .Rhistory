port="5433"
)
cur = conn.cursor()
#conn.rollback()  # ‚ùó Clear the broken transaction state
import os
import sys
import psycopg2
import pdfplumber
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores.pgvector import PGVector
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_community.llms import Ollama
from langchain.memory import ConversationBufferMemory
sys.stdout.reconfigure(encoding='utf-8')
with pdfplumber.open("your_pdf_file.pdf") as pdf:
text = "\n\n".join(p.extract_text() for p in pdf.pages if p.extract_text())
chunks = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=80).split_text(text)
model = SentenceTransformer("all-MiniLM-L6-v2")
conn = psycopg2.connect(dbname="db5", user="postgres", password="postgres", host="localhost", port="5433")
cur = conn.cursor()
cur.execute("""
CREATE TABLE IF NOT EXISTS minilm (
id SERIAL PRIMARY KEY,
content TEXT,
embedding VECTOR(384)
)
""")
conn.commit()
for chunk in chunks:
cur.execute("INSERT INTO minilm (content, embedding) VALUES (%s, %s)", (chunk, model.encode(chunk).tolist()))
conn.commit()
retriever = PGVector(
collection_name="minilm",
connection_string="postgresql+psycopg2://postgres:postgres@localhost:5433/db5",
embedding_function=HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
).as_retriever(search_kwargs={"k": 5})
qa = RetrievalQA.from_chain_type(
llm=Ollama(model="llama3", temperature=0.1),
retriever=retriever,
memory=ConversationBufferMemory(memory_key="chat_history", return_messages=True),
return_source_documents=True,
output_key="result"
)
while (q := input("Ask a question (or type 'exit'): ").strip().lower()) != "exit":
print("\nAnswer:\n", qa.invoke({"query": q})["result"])
import os
import sys
import psycopg2
import pdfplumber
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores.pgvector import PGVector
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_community.llms import Ollama
from langchain.memory import ConversationBufferMemory
sys.stdout.reconfigure(encoding='utf-8')
with pdfplumber.open("fusion.pdf") as pdf:
text = "\n\n".join(p.extract_text() for p in pdf.pages if p.extract_text())
chunks = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=80).split_text(text)
model = SentenceTransformer("all-MiniLM-L6-v2")
conn = psycopg2.connect(dbname="db5", user="postgres", password="postgres", host="localhost", port="5433")
cur = conn.cursor()
cur.execute("""
CREATE TABLE IF NOT EXISTS minilm (
id SERIAL PRIMARY KEY,
content TEXT,
embedding VECTOR(384)
)
""")
conn.commit()
for chunk in chunks:
cur.execute("INSERT INTO minilm (content, embedding) VALUES (%s, %s)", (chunk, model.encode(chunk).tolist()))
conn.commit()
retriever = PGVector(
collection_name="minilm",
connection_string="postgresql+psycopg2://postgres:postgres@localhost:5433/db5",
embedding_function=HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
).as_retriever(search_kwargs={"k": 5})
qa = RetrievalQA.from_chain_type(
llm=Ollama(model="llama3", temperature=0.1),
retriever=retriever,
memory=ConversationBufferMemory(memory_key="chat_history", return_messages=True),
return_source_documents=True,
output_key="result"
)
while (q := input("Ask a question (or type 'exit'): ").strip().lower()) != "exit":
print("\nAnswer:\n", qa.invoke({"query": q})["result"])
fusion
qa = RetrievalQA.from_chain_type(
llm=Ollama(model="llama3", temperature=0.1),
retriever=retriever,
memory=ConversationBufferMemory(memory_key="chat_history", return_messages=True),
return_source_documents=True,
output_key="result"
)
output_key
qa = RetrievalQA.from_chain_type(
llm=Ollama(model="llama3", temperature=0.1),
retriever=retriever,
memory=ConversationBufferMemory(memory_key="chat_history", return_messages=True),
return_source_documents=True,
output_key="result"
)
qa.invoke({"query": q})["result"]
import sys
print("Running in:", sys.executable)
import os
os.chdir("C:/Users/ADMIN/Desktop/env6")
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
import json
from langchain.text_splitter import RecursiveCharacterTextSplitter
import openai
import os
import pdfplumber
import langchain_community
from langchain_community.vectorstores import PGVector
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema.document import Document
import psycopg2
openai.api_key = os.getenv("OPENAI_API_KEY")
import json
pdf_path = "fusion.pdf"
pdf_path = "pranathiss_knowledge_base.pdf"
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import PGVector
from langchain_community.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain_community.llms import Ollama
from langchain.memory import ConversationBufferMemory
from langchain_ollama import OllamaLLM
conn = psycopg2.connect(
dbname="db5",
user="postgres",
password="postgres",
host="localhost",
port="5433"
)
cur = conn.cursor()
#conn.rollback()  # ‚ùó Clear the broken transaction state
import os
import sys
import psycopg2
import pdfplumber
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores.pgvector import PGVector
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_community.llms import Ollama
from langchain.memory import ConversationBufferMemory
sys.stdout.reconfigure(encoding='utf-8')
with pdfplumber.open("fusion.pdf") as pdf:
text = "\n\n".join(p.extract_text() for p in pdf.pages if p.extract_text())
chunks = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=80).split_text(text)
model = SentenceTransformer("all-MiniLM-L6-v2")
conn = psycopg2.connect(dbname="db5", user="postgres", password="postgres", host="localhost", port="5433")
cur = conn.cursor()
cur.execute("""
CREATE TABLE IF NOT EXISTS minilm (
id SERIAL PRIMARY KEY,
content TEXT,
embedding VECTOR(384)
)
""")
conn.commit()
for chunk in chunks:
cur.execute("INSERT INTO minilm (content, embedding) VALUES (%s, %s)", (chunk, model.encode(chunk).tolist()))
conn.commit()
retriever = PGVector(
collection_name="minilm",
connection_string="postgresql+psycopg2://postgres:postgres@localhost:5433/db5",
embedding_function=HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
).as_retriever(search_kwargs={"k": 5})
qa = RetrievalQA.from_chain_type(
llm=Ollama(model="llama3", temperature=0.1),
retriever=retriever,
memory=ConversationBufferMemory(memory_key="chat_history", return_messages=True),
return_source_documents=True,
output_key="result"
)
qa.invoke({"query": q})["result"]
qa = RetrievalQA.from_chain_type(
llm=Ollama(model="llama3", temperature=0.1),
chain_type="stuff",  # or "map_reduce", etc.
retriever=retriever,
return_source_documents=True
)
qa.invoke({"query": q})["result"]
while (q := input("Ask a question (or type 'exit'): ").strip().lower()) != "exit":
print("\nAnswer:\n", qa.invoke({"query": q})["result"])
fusion
what is fusion
whata re the 3 conditions for nuclear fusion
exit()
exit
from langchain.chains import ConversationalRetrievalChain
import ConversationalRetrievalChain as np
from langchain.chains import ConversationalRetrievalChain
import ConversationalRetrievalChain as np
import sys
print("Running in:", sys.executable)
import os
os.chdir("C:/Users/ADMIN/Desktop/env6")
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
import json
from langchain.text_splitter import RecursiveCharacterTextSplitter
import openai
import os
import pdfplumber
import langchain_community
from langchain_community.vectorstores import PGVector
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema.document import Document
import psycopg2
openai.api_key = os.getenv("OPENAI_API_KEY")
import json
pdf_path = "fusion.pdf"
pdf_path = "pranathiss_knowledge_base.pdf"
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import PGVector
from langchain_community.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain_community.llms import Ollama
from langchain.memory import ConversationBufferMemory
from langchain_ollama import OllamaLLM
from langchain.chains import ConversationalRetrievalChain
conn = psycopg2.connect(
dbname="db5",
user="postgres",
password="postgres",
host="localhost",
port="5433"
)
cur = conn.cursor()
#conn.rollback()  # ‚ùó Clear the broken transaction state
memory = ConversationBufferMemory(
memory_key="chat_history",  # Required key
return_messages=True        # Important for conversational chains
)
qa = RetrievalQA.from_chain_type(
llm=Ollama(model="llama3", temperature=0.1),
chain_type="stuff",  # or "map_reduce", etc.
retriever=retriever,
memory=memory,
return_source_documents=True
)
qa.invoke({"query": q})["result"]
qa = RetrievalQA.from_chain_type(
llm=Ollama(model="llama3", temperature=0.1),
retriever=retriever,
memory=ConversationBufferMemory(
memory_key="chat_history",
return_messages=True
),
chain_type="stuff",  # or whatever you're using
return_source_documents=True,
output_key="result"  # üëà THIS fixes the error!
)
qa.invoke({"query": q})["result"]
with pdfplumber.open("fusion.pdf") as pdf:
text = "\n\n".join(p.extract_text() for p in pdf.pages if p.extract_text())
chunks = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=80).split_text(text)
model = SentenceTransformer("all-MiniLM-L6-v2")
conn = psycopg2.connect(dbname="db5", user="postgres", password="postgres", host="localhost", port="5433")
cur = conn.cursor()
cur.execute("""
CREATE TABLE IF NOT EXISTS minilm (
id SERIAL PRIMARY KEY,
content TEXT,
embedding VECTOR(384)
)
""")
conn.commit()
for chunk in chunks:
cur.execute("INSERT INTO minilm (content, embedding) VALUES (%s, %s)", (chunk, model.encode(chunk).tolist()))
conn.commit()
retriever = PGVector(
collection_name="minilm",
connection_string="postgresql+psycopg2://postgres:postgres@localhost:5433/db5",
embedding_function=HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
).as_retriever(search_kwargs={"k": 5})
memory = ConversationBufferMemory(
memory_key="chat_history",
return_messages=True,
output_key="answer"  # this is required!
)
qa = ConversationalRetrievalChain.from_llm(
llm=llm,
retriever=retriever,
memory=memory,
return_source_documents=True,
output_key="answer"  # tell LangChain what to store
)
qa.invoke({"query": q})["result"]
while (q := input("Ask a question (or type 'exit'): ").strip().lower()) != "exit":
print("\nAnswer:\n", qa.invoke({"query": q})["result"])
exit()
while (q := input("Ask a question (or type 'exit'): ").strip().lower()) != "exit":
print("\nAnswer:\n", qa.invoke({"query": q})["result"])
fusion
print("\nAnswer:\n", qa.invoke({"question": q})["answer"])
while (q := input("Ask a question (or type 'exit'): ").strip().lower()) != "exit":
print("\nAnswer:\n", qa.invoke({"query": q})["result"])
exit
while (q := input("Ask a question (or type 'exit'): ").strip().lower()) != "exit":
print("\nAnswer:\n", qa.invoke({"query": q})["result"])
fusion
memory = ConversationBufferMemory(
memory_key="chat_history",
return_messages=True,
output_key="answer"  # this is required!
)
qa = ConversationalRetrievalChain.from_llm(
llm=llm,
retriever=retriever,
memory=memory,
return_source_documents=True,
output_key="answer"  # tell LangChain what to store
)
print("\nAnswer:\n", qa.invoke({"question": q})["answer"])
while (q := input("Ask a question (or type 'exit'): ").strip().lower()) != "exit":
print("\nAnswer:\n", qa.invoke({"query": q})["result"])
fusion
memory = ConversationBufferMemory(
memory_key="chat_history",
return_messages=True,
output_key="answer"  # this is required!
)
qa = ConversationalRetrievalChain.from_llm(
llm=Ollama(model="llama3", temperature=0.1),
retriever=retriever,  # <- this should be a valid retriever
memory=memory,
return_source_documents=True,
output_key="answer"  # also IMPORTANT!
)
while (q := input("Ask a question (or type 'exit'): ").strip().lower()) != "exit":
print("\nAnswer:\n", qa.invoke({"query": q})["result"])
fusion
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.vectorstores.pgvector import PGVector
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import Ollama
# Setup retriever (your PGVector config)
retriever = PGVector(
collection_name="minilm",
connection_string="postgresql+psycopg2://postgres:postgres@localhost:5433/db5",
embedding_function=HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
).as_retriever(search_kwargs={"k": 5})
# Setup memory
memory = ConversationBufferMemory(
memory_key="chat_history",
return_messages=True,
output_key="answer"
)
# Setup LLM
llm = Ollama(model="llama3", temperature=0.1)
# Build Conversational Chain
qa_chain = ConversationalRetrievalChain.from_llm(
llm=llm,
retriever=retriever,
memory=memory,
return_source_documents=True,
output_key="answer"
)
# Check the expected input keys
print("Expected input keys:", qa_chain.input_keys)
# Chat loop
while True:
question = input("Ask a question (or type 'exit'): ").strip()
if question.lower() == "exit":
break
result = qa_chain.invoke({"question": question})
print("\nAnswer:\n", result["answer"])
fusion
what is nuclear fusion
what are the three conditions for nucelar fusion
LICF is laser inertial confinement fusion
what is LICF
of what isotopes for the high temperature?
dont you remember the conditions?
exit
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.llms import Ollama
from langchain.vectorstores.pgvector import PGVector
from langchain.embeddings import HuggingFaceEmbeddings
# Setup retriever
retriever = PGVector(
collection_name="minilm",
connection_string="postgresql+psycopg2://postgres:postgres@localhost:5433/db5",
embedding_function=HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
).as_retriever()
# Setup memory
memory = ConversationBufferMemory(
memory_key="chat_history",
return_messages=True,
output_key="answer"
)
# LLM
llm = Ollama(model="llama3", temperature=0.1)
# Chain with memory
qa = ConversationalRetrievalChain.from_llm(
llm=llm,
retriever=retriever,
memory=memory,
return_source_documents=False,
output_key="answer"
)
# Sanity check
print("Chain type:", type(qa))
print("Input keys:", qa.input_keys)
import sys
print("Running in:", sys.executable)
import os
os.chdir("C:/Users/ADMIN/Desktop/env6")
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
import json
from langchain.text_splitter import RecursiveCharacterTextSplitter
import openai
import os
import pdfplumber
import langchain_community
from langchain_community.vectorstores import PGVector
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema.document import Document
import psycopg2
openai.api_key = os.getenv("OPENAI_API_KEY")
import json
pdf_path = "fusion.pdf"
pdf_path = "pranathiss_knowledge_base.pdf"
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import PGVector
from langchain_community.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain_community.llms import Ollama
from langchain.memory import ConversationBufferMemory
from langchain_ollama import OllamaLLM
from langchain.chains import ConversationalRetrievalChain
conn = psycopg2.connect(
dbname="db5",
user="postgres",
password="postgres",
host="localhost",
port="5433"
)
cur = conn.cursor()
#conn.rollback()  # ‚ùó Clear the broken transaction state
# Setup retriever
retriever = PGVector(
collection_name="minilm",
connection_string="postgresql+psycopg2://postgres:postgres@localhost:5433/db5",
embedding_function=HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
).as_retriever()
# Setup memory
memory = ConversationBufferMemory(
memory_key="chat_history",
return_messages=True,
output_key="answer"
)
# LLM
llm = Ollama(model="llama3", temperature=0.1)
# Chain with memory
qa = ConversationalRetrievalChain.from_llm(
llm=llm,
retriever=retriever,
memory=memory,
return_source_documents=False,
output_key="answer"
)
# Sanity check
print("Chain type:", type(qa))
print("Input keys:", qa.input_keys)
# Interactive loop
while True:
question = input("Ask a question (or type 'exit'): ").strip()
if question.lower() == "exit":
break
result = qa.invoke({"question": question})
print("\nAnswer:\n", result["answer"])
# Show memory after each turn
print("\n--- Memory so far ---")
for msg in memory.chat_memory.messages:
print(f"{msg.type}: {msg.content}")
print("---------------------\n")
what are the 3 conditions for nuclear fusion
what is the first condition
how many million degree celcius?
isotopes of which
what conditions are sufficient
what are its benefits over fission
exit
cur.execute("""
CREATE TABLE IF NOT EXISTS chat_memory (
id SERIAL PRIMARY KEY,
content TEXT NOT NULL,
embedding VECTOR(384) NOT NULL,
timestamp TIMESTAMP DEFAULT NOW()
);
""")
